{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TZKKmXqmgD1"
   },
   "source": [
    "https://www.kaggle.com/datasets/ethon0426/lending-club-20072020q1?select=Loan_status_2007-2020Q3.gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwTSd_R7loD-",
    "outputId": "f17f3d81-824a-4f72-e15d-b731f366cd08"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "# import kagglehub\n",
    "# ethon0426_lending_club_20072020q1_path = kagglehub.dataset_download('ethon0426/lending-club-20072020q1')\n",
    "\n",
    "# print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": true,
    "id": "iAegNPnrloEA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SPMDU8Q552Du"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "id": "lmE9bXtN575u",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(ethon0426_lending_club_20072020q1_path+'/Loan_status_2007-2020Q3.gzip', low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoan_status_2007-2020Q3.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/jupyterhub/anaconda/envs/anaconda2024.02/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/jupyterhub/anaconda/envs/anaconda2024.02/lib/python3.12/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/jupyterhub/anaconda/envs/anaconda2024.02/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/jupyterhub/anaconda/envs/anaconda2024.02/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32mparsers.pyx:825\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:913\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"Loan_status_2007-2020Q3.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPZL4Lfs7up8"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_A3zsSw8E1N"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp7k8FU_8SPS"
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qqlUAi1_v0T"
   },
   "outputs": [],
   "source": [
    "cleaning = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WR5rQfEgABZj"
   },
   "outputs": [],
   "source": [
    "nan_by_col = cleaning.isna().sum().reset_index()\n",
    "nan_by_col.columns = [\"columns\", \"missing\"]\n",
    "nan_by_col = nan_by_col.set_index(\"columns\")\n",
    "print(nan_by_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PmJzJdJAJw6"
   },
   "outputs": [],
   "source": [
    "nan_by_col[\"valid\"] = [len(cleaning) - c for c in nan_by_col[\"missing\"]]\n",
    "nan_by_col[\"missing%\"] = (cleaning.isna().sum()/cleaning.shape[0])*100\n",
    "print(nan_by_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF5Fyb_mAjEM"
   },
   "outputs": [],
   "source": [
    "g = nan_by_col[[\"missing\", \"valid\"]].plot(kind = \"barh\", stacked= \"true\")\n",
    "g.set_xlabel(\"Nr.rows\")\n",
    "g.set_title(\"Number of missing values by column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knexw0cdBEwL"
   },
   "outputs": [],
   "source": [
    "num_of_cells_with_nan = cleaning.isna().sum().sum()\n",
    "print(f\"Cells with NaN: {num_of_cells_with_nan}\")\n",
    "min_one_nan = cleaning.isna().any(axis = 1).sum()\n",
    "print(f\"Rows with at least one NaN: {min_one_nan}\")\n",
    "all_nan = cleaning.isna().all(axis = 1).sum()\n",
    "print(f\"Rows with all NaN: {all_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdypGh76BNQb"
   },
   "outputs": [],
   "source": [
    "percent_missing_per_row = (cleaning.isna().sum(axis = 1) / len(cleaning.columns))*100\n",
    "percent_missing_per_row.hist(figsize = (5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzstkqR5DroU"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = cleaning.columns[cleaning.notna().sum() < 282076]\n",
    "print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_XXQlBOBZRd"
   },
   "outputs": [],
   "source": [
    "cleaning.dropna(axis=1, thresh=2910000 , inplace=True) # Remove columns with less than 10 non-missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuqcwH0cCu4S"
   },
   "outputs": [],
   "source": [
    "cleaning.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8Uzco0OEsxX"
   },
   "outputs": [],
   "source": [
    "nan_by_col = cleaning.isna().sum().reset_index()\n",
    "nan_by_col.columns = [\"columns\", \"missing\"]\n",
    "nan_by_col = nan_by_col.set_index(\"columns\")\n",
    "print(nan_by_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bvy5_bUsJOYm"
   },
   "outputs": [],
   "source": [
    "cleaning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhSzYYAPLMqm"
   },
   "outputs": [],
   "source": [
    "for col in cleaning.columns:\n",
    "    \n",
    "    print(cleaning[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNr82BmbF--h"
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "iter_imputer = IterativeImputer()\n",
    "iter_data = cleaning.copy()\n",
    "\n",
    "# Select only numerical features for imputation\n",
    "numerical_columns = iter_data.select_dtypes(include=['number']).columns\n",
    "numerical_data = iter_data[numerical_columns]\n",
    "\n",
    "# Perform imputation on numerical data only\n",
    "iter_data[numerical_columns] = iter_imputer.fit_transform(numerical_data)\n",
    "\n",
    "#iter_data = pd.DataFrame(iter_data, columns=cleaning.columns, index=cleaning.index) # No need to recreate DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBHKGR0UGbdO"
   },
   "outputs": [],
   "source": [
    "# #should I still use KNN imputer with so many outliers ? Should I remove the outliers first?\n",
    "# from sklearn.impute import KNNImputer\n",
    "\n",
    "# knn_imputer = KNNImputer(n_neighbors = 5)\n",
    "# knn_data = cleaning.copy()\n",
    "# numerical_columns = knn_data.select_dtypes(include=['number']).columns\n",
    "# numerical_data = knn_data[numerical_columns]\n",
    "# knn_data[numerical_columns] = knn_imputer.fit_transform(numerical_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_a1wapUZSDo"
   },
   "outputs": [],
   "source": [
    "# # two histplot to compare the imputed data (knn_data and iter_data)  on top of each other for features \"MonthlyIncome\",\"NumberOfDependents\"\n",
    "\n",
    "# fig, ax = plt.subplots(nrows= 2, figsize=(10, 10))\n",
    "# sns.histplot(knn_data[\"MonthlyIncome\"], ax=ax[0], kde=True, label = 'knn')\n",
    "# sns.histplot(iter_data[\"MonthlyIncome\"], ax=ax[0], kde=True, label = 'iterative')\n",
    "# ax[0].set_title(\"monthly income\")\n",
    "# ax[0].legend()\n",
    "# ax[0].set_ylim(0, 15000)\n",
    "# ax[0].set_xlim(0, 20000)\n",
    "# sns.histplot(knn_data[\"NumberOfDependents\"], ax=ax[1], kde=True, label = 'knn')\n",
    "# sns.histplot(iter_data[\"NumberOfDependents\"], ax=ax[1], kde=True, label = 'iterative')\n",
    "# ax[1].set_title(\"NumberOfDependents\")\n",
    "# ax[1].legend()\n",
    "\n",
    "# plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PmJzJdJAJw6"
   },
   "outputs": [],
   "source": [
    "nan_by_col[\"valid\"] = [len(iter_data) - c for c in nan_by_col[\"missing\"]]\n",
    "nan_by_col[\"missing%\"] = (iter_data.isna().sum()/cleaning.shape[0])*100\n",
    "print(nan_by_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_by_col[\"valid\"] = [len(iter_data) - c for c in nan_by_col[\"missing\"]]\n",
    "nan_by_col[\"missing%\"] = (iter_data.isna().sum()/cleaning.shape[0])*100\n",
    "print(nan_by_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = iter_data[numerical_columns].quantile(0.25)\n",
    "Q3 = iter_data[numerical_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_condition = ((iter_data[numerical_columns] < (Q1 - 1.5 * IQR)) | (iter_data[numerical_columns] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "outlier_indices = iter_data[outlier_condition].index \n",
    "iter_data.drop(outlier_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = iter_data.copy()\n",
    "#if loan_status = current or issued, drop the row\n",
    "clean_data = clean_data[~clean_data['loan_status'].isin(['Current', 'Issued'])]\n",
    "clean_data['loan_status'] = iter_data['loan_status'].apply(lambda x: 0 if x in ['Fully Paid'] else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.drop(columns=['id', 'url', 'pymnt_plan','Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_object = clean_data.select_dtypes(include=['object'])\n",
    "df_object.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_object.columns :\n",
    "        print(clean_data[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_data['term'] = clean_data['term'].str.split(' ').str[0].astype(int)\n",
    "clean_data['term'] = clean_data['term'].str.extract(r'(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['int_rate'] = clean_data['int_rate'].str.rstrip('%').astype(float) / 100\n",
    "clean_data['revol_util'] = iter_data['revol_util'].str.rstrip('%').astype(float) / 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['revol_util'] = iter_data['revol_util'].str.rstrip('%').astype(float) / 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['initial_list_status'] = np.where(clean_data['initial_list_status'] == 'w', 1, 0)\n",
    "clean_data['debt_settlement_flag'] = np.where(clean_data['debt_settlement_flag'] == 'Y', 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']\n",
    "for col in date_columns:\n",
    "    clean_data[col] = pd.to_datetime(clean_data[col], format='%b-%Y')  # Adjust format if needed\n",
    "clean_data['time_since_earliest_cr_line'] = (clean_data['issue_d'] - clean_data['earliest_cr_line']).dt.days\n",
    "clean_data['time_since_last_pymnt'] = (clean_data['issue_d'] - clean_data['last_pymnt_d']).dt.days\n",
    "clean_data['days_since_last_credit_pull'] = (pd.Timestamp.now() - clean_data['last_credit_pull_d']).dt.days\n",
    "clean_data = clean_data.drop(columns=date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_data.drop(columns=['pymnt_plan', 'Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.drop(columns=['zip_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = clean_data['loan_status'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = clean_data.select_dtypes(include=['object']).columns.tolist()  \n",
    "# Strip whitespace and replace empty strings with NaN\n",
    "clean_data[object_cols] = clean_data[object_cols].apply(lambda x: x.str.strip().replace('', np.nan))\n",
    "\n",
    "# Now check for nulls again\n",
    "print(clean_data[object_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded2 = pd.get_dummies(clean_data.drop(columns=['loan_status']), \n",
    "                       columns=object_cols, \n",
    "                       dummy_na=True)\n",
    "print(np.isnan(encoded2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoded = clean_data.copy()\n",
    "encoded.drop(columns=['loan_status'], inplace= True)\n",
    "# encoded = encoded.drop(columns=['loan_status'])\n",
    "# # Assuming 'df' is your DataFrame\n",
    "# \n",
    "\n",
    "# # Create a OneHotEncoder object\n",
    "# encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  \n",
    "\n",
    "# # Fit the encoder to your categorical features\n",
    "# encoded_data = encoder.fit_transform(encoded[object_cols])\n",
    "\n",
    "# # Create a DataFrame from the encoded data\n",
    "# encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(object_cols))\n",
    "\n",
    "# # Concatenate the encoded DataFrame with the original DataFrame\n",
    "# encoded = pd.concat([encoded, encoded_df], axis=1)\n",
    "\n",
    "# # Drop the original categorical features \n",
    "# encoded = encoded.drop(columns=object_cols)\n",
    "# Ensure all categorical columns are strings\n",
    "encoded[object_cols] = encoded[object_cols].astype(str)\n",
    "\n",
    "# Reinitialize encoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_data = encoder.fit_transform(encoded[object_cols])\n",
    "\n",
    "# Ensure index alignment\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(object_cols), index=encoded.index)\n",
    "\n",
    "# Drop original columns and concatenate\n",
    "encoded = pd.concat([encoded.drop(columns=object_cols), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded.to_csv('my_dataframe.csv', index=False)\n",
    "p2p_loan_data = pd.concat([Y,encoded], axis=1)\n",
    "p2p_loan_data.to_csv('p2p_loan_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove features with <1% variance\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_filtered = selector.fit_transform(encoded)\n",
    "var_filtered = pd.DataFrame(X_filtered, columns = selector.get_feature_names_out())\n",
    "# Assuming 'df' is your DataFrame\n",
    "variances = var_filtered.var().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 60))  # Adjust figure size as needed\n",
    "plt.barh(variances.index, variances.values)\n",
    "plt.title('Feature Variances')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Drop highly correlated numerical features (Pearson |corr| > 0.9)\n",
    "corr_matrix = var_filtered.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n",
    "corr_filtered = var_filtered.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = corr_filtered.corr()\n",
    "plt.figure(figsize=(10, 8))  # Adjust figure size as needed\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('Correlation Matrix.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2,mutual_info_classif\n",
    "\n",
    "# Select top 20 numerical features\n",
    "selector = SelectKBest(mutual_info_classif, k=20)\n",
    "X_selected = selector.fit_transform(corr_filtered, Y)  # X_num = numerical subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = selector.scores_\n",
    "feature_names = corr_filtered.columns\n",
    "\n",
    "# Create a DataFrame for easy sorting\n",
    "feature_scores = pd.DataFrame({'Feature': feature_names, 'Score': scores})\n",
    "feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
    "\n",
    "# Plot (horizontal bar chart)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_scores['Feature'][:20], feature_scores['Score'][:20], color='skyblue')\n",
    "plt.xlabel('Importance Score (Mutual Information)')\n",
    "plt.title('Top 20 Features by Mutual Information Score')\n",
    "plt.gca().invert_yaxis()  # Highest score at top\n",
    "plt.savefig('Top 20 Features by MI filtered.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(mutual_info_classif, k=20)\n",
    "X_selected = selector.fit_transform(encoded, Y) \n",
    "selector.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = selector.scores_\n",
    "feature_names = encoded.columns\n",
    "\n",
    "# Create a DataFrame for easy sorting\n",
    "feature_scores = pd.DataFrame({'Feature': feature_names, 'Score': scores})\n",
    "feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
    "\n",
    "# Plot (horizontal bar chart)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_scores['Feature'][:20], feature_scores['Score'][:20], color='skyblue')\n",
    "plt.xlabel('Importance Score (Mutual Information)')\n",
    "plt.title('Top 20 Features by Mutual Information Score')\n",
    "plt.gca().invert_yaxis()  # Highest score at top\n",
    "plt.savefig('Top 20 Features by MI encoded.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = pd.DataFrame(X_selected, columns= selector.get_feature_names_out())\n",
    "correlation_matrix = X_selected.corr()\n",
    "plt.figure(figsize=(10, 8))  # Adjust figure size as needed\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('Correlation Matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)\n",
    "# model.fit(corr_filtered, Y)\n",
    "# selected_features = corr_filtered.columns[model.coef_[0] != 0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in selected_features:\n",
    "#     i = 0\n",
    "#     if model.coef_[i] != 0\n",
    "#         print(feature, model.coef_[i])\n",
    "#     i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "rfe = RFECV(model, min_features_to_select=10, cv=5)\n",
    "rfe.fit(encoded, Y)\n",
    "selected_features = encoded.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV scores vs. number of features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rfe.cv_results_['mean_test_score']) + 1),\n",
    "         rfe.cv_results_['mean_test_score'],\n",
    "         marker='o',\n",
    "         linestyle='--',\n",
    "         color='b')\n",
    "\n",
    "# Highlight the optimal number of features\n",
    "optimal_features = rfe.n_features_\n",
    "plt.axvline(x=optimal_features, color='r', linestyle=':',\n",
    "            label=f'Optimal: {optimal_features} features')\n",
    "\n",
    "plt.xlabel('Number of Features Selected')\n",
    "plt.ylabel('Mean CV Score (Accuracy)')\n",
    "plt.title('RFECV: Optimal Number of Features')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('RFECV_scores.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of feature rankings\n",
    "rankings = pd.DataFrame({\n",
    "    'Feature': encoded.columns,\n",
    "    'Rank': rfe.ranking_,\n",
    "    'Selected': rfe.support_\n",
    "}).sort_values('Rank')\n",
    "\n",
    "# Plot (horizontal bar chart)\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = np.where(rankings['Selected'], 'green', 'red')  # Green=selected, Red=dropped\n",
    "plt.barh(rankings['Feature'], rankings['Rank'], color=colors)\n",
    "plt.xlabel('Feature Rank (1=Selected)')\n",
    "plt.title('RFECV Feature Rankings')\n",
    "plt.gca().invert_yaxis()  # Highest rank (1) at top\n",
    "plt.grid(axis='x')\n",
    "\n",
    "# Add legend\n",
    "import matplotlib.patches as mpatches\n",
    "green_patch = mpatches.Patch(color='green', label='Selected')\n",
    "red_patch = mpatches.Patch(color='red', label='Dropped')\n",
    "plt.legend(handles=[green_patch, red_patch])\n",
    "plt.savefig('RFECV_rankings.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: CV Scores\n",
    "ax1.plot(range(1, len(rfe.cv_results_['mean_test_score']) + 1), \n",
    "         rfe.cv_results_['mean_test_score'], \n",
    "         marker='o', color='b')\n",
    "ax1.axvline(optimal_features, color='r', linestyle=':', \n",
    "            label=f'Optimal: {optimal_features} features')\n",
    "ax1.set_title('Optimal Feature Count', fontsize=14)\n",
    "ax1.set_xlabel('Number of Features')\n",
    "ax1.set_ylabel('CV Score')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Feature Rankings\n",
    "colors = np.where(rankings['Selected'], '#1f77b4', '#ff7f0e')  # Blue=selected, Orange=dropped\n",
    "ax2.barh(rankings['Feature'], rankings['Rank'], color=colors)\n",
    "ax2.set_title('Feature Selection Rankings', fontsize=14)\n",
    "ax2.set_xlabel('Rank (1=Selected)')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('RFECV_Results.png', dpi=300, bbox_inches='tight')  # Save for slides\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 772636,
     "sourceId": 1747344,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.12 (Anaconda 2024.02)",
   "language": "python",
   "name": "anaconda2024.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
